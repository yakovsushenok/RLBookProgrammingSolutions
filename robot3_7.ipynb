{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "robot3_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNItiS1bUylVx4rVG5JLKU/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yakovsushenok/RLBookProgrammingSolutions/blob/main/robot3_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to write this code to check how exactly an agent behaves when the rewards for transitions in a maze are equal to $0$ and the reward for escaping the maze is $+1$.\n",
        "\n",
        "This is to answer the following question from Sutton and Barto's book:\n",
        "\n",
        "Exercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a\n",
        "reward of +1 for escaping from the maze and a reward of zero at all other times. The task\n",
        "seems to break down naturally into episodes—the successive runs through the maze—so\n",
        "you decide to treat it as an episodic task, where the goal is to maximize expected total\n",
        "reward (3.7). After running the learning agent for a while, you find that it is showing\n",
        "no improvement in escaping from the maze. What is going wrong? Have you e↵ectively\n",
        "communicated to the agent what you want it to achieve?"
      ],
      "metadata": {
        "id": "1U5EhrZVR-Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's create a matrix which will represent our maze. The 2 corners at the top left and bottom right are the exits of the maze. If the agent steps into the exit, then the episode terminates."
      ],
      "metadata": {
        "id": "J2F9NHucTggJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def maze(s):\n",
        "  maze = np.zeros((s, s))\n",
        "  maze[0,0], maze[-1,-1] = 1, 1\n",
        "  return maze"
      ],
      "metadata": {
        "id": "hfGB73NzSJTQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how a $4 \\times 4$ maze looks like:"
      ],
      "metadata": {
        "id": "_urS1xS3SNNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(maze(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQUnSO8xSP72",
        "outputId": "61b7f40e-6a44-427f-cb3a-e3ce3f7a39f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's calculate the state values for the uniform policy. Note that if you're on the edge and you turn into the wall then you circle back to the same state. \n",
        "\n",
        "Under the uniform policy each action has a probability of $\\frac{1}{4}$ to be taken. We calculate the state values under the uniform policy because it's interesting to see what will the estimates be with such rewards. The algorithm for evaluating the state values is:\n",
        "\n",
        "First, initialise $v_0=0$, then iterate $$\\forall s: v_{k+1}(s)\\leftarrow E[R_{t+1}+\\gamma v_k (S_{t+1})|s,\\pi ]$$\n",
        "\n"
      ],
      "metadata": {
        "id": "5IjK-UCnUNGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findCells(index, grid):\n",
        "  \"\"\"\n",
        "  A function which will get the cells values neighboring a specific cell. \n",
        "  input:\n",
        "      index: array of 2 entries i and j which indicate the cell's position. i is the row and j is the column\n",
        "  ouput: \n",
        "      cells: 2 arrays. The first array has the row values of the neighbouring cells and the second array the column values. \n",
        "  Example:\n",
        "   input: [1,1]\n",
        "   output: [1, 0, 1, 2], [0, 1, 2, 1]\n",
        "  \"\"\"\n",
        "  i, j = index[0], index[1]\n",
        "  # Terminal state\n",
        "  if (np.all(grid[i, :] == grid[0, :]) and np.all(grid[:, j] == grid[:, 0])) or (np.all(grid[i, :] == grid[-1, :]) and np.all(grid[:, j] == grid[:, -1])):\n",
        "    return None, None\n",
        "  # Top wall, not corner\n",
        "  elif np.all(grid[i, :] == grid[0, :]) and np.all(grid[:, j] == grid[:,-1]) == False:\n",
        "    return [i, i, i, i+1], [j-1, j, j+1, j]\n",
        "  # Top wall, corner\n",
        "  elif np.all(grid[i, :] == grid[0, :]) and np.all(grid[:, j] == grid[:,-1]):\n",
        "    return [i, i, i, i+1], [j-1, j, j, j]\n",
        "  # Bottom wall, not corner\n",
        "  elif np.all(grid[i, :] == grid[-1, :]) and np.all(grid[:, j] == grid[:, 0])==False:\n",
        "    return [i, i-1, i, i], [j-1, j, j+1, j]\n",
        "  # Bottom wall, corner\n",
        "  elif np.all(grid[i, :] == grid[-1, :]) and np.all(grid[:, j] == grid[:, 0]):\n",
        "    return [i, i-1, i, i], [j, j, j+1, j]\n",
        "  # Left wall, not corner\n",
        "  elif np.all(grid[:, j] == grid[:, 0]) and np.all(grid[i,:]== grid[-1,:])==False:\n",
        "    return [i, i-1, i, i+1], [j, j, j+1, j]\n",
        "  # Right wall, not corner\n",
        "  elif np.all(grid[:, j] == grid[:, -1]) and np.all(grid[i, :] == grid[-1,:])==False:\n",
        "    return [i, i-1, i, i+1], [j-1, j, j, j]\n",
        "  # No wall\n",
        "  else:\n",
        "    return [i, i-1, i, i+1], [j-1, j, j+1, j]\n",
        "  \n",
        "def robot_experiment(iterations, discount, maze_size):\n",
        "  ITERATIONS = iterations\n",
        "  grid = maze(maze_size)\n",
        "  gridCollection = [maze(4)]\n",
        "  probs = np.array([0.25, 0.25, 0.25, 0.25])\n",
        "  for iter in range(ITERATIONS):\n",
        "      for row in range(grid.shape[0]):\n",
        "        for col in range(grid.shape[1]):\n",
        "          index = [row, col]\n",
        "          rowID, colID = findCells(index, grid)\n",
        "          if rowID == None or colID == None:\n",
        "            grid[row, col] = grid[row, col]\n",
        "          else:\n",
        "            last_grid = gridCollection[iter]\n",
        "            grid[row, col] = discount*last_grid[rowID, colID] @ probs\n",
        "      new_grid = grid.copy()\n",
        "      gridCollection.append(new_grid)\n",
        "      if (iter+1)% iterations==0:\n",
        "        print(f\"Iteration: {iter+1}, grid:\")\n",
        "        print(np.round(grid, 4))\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "LBTBdtOpTKnO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am now going to see what the state values converge to. I will try values with a discount $\\gamma \\in \\{0.1,0.9,1\\}$. I will do 200 iterations as it's enough for convergence."
      ],
      "metadata": {
        "id": "FSkw0YMxPrp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "robot_experiment(200, 0.1, 4)\n",
        "robot_experiment(200, 0.9, 4)\n",
        "robot_experiment(200, 1, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnwlltGOP4Sy",
        "outputId": "659eb5d6-e56b-4886-9920-9d42afbf4229"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 200, grid:\n",
            "[[1.00e+00 2.57e-02 7.00e-04 0.00e+00]\n",
            " [2.57e-02 1.30e-03 1.00e-04 7.00e-04]\n",
            " [7.00e-04 1.00e-04 1.30e-03 2.57e-02]\n",
            " [0.00e+00 7.00e-04 2.57e-02 1.00e+00]]\n",
            "\n",
            "\n",
            "Iteration: 200, grid:\n",
            "[[1.     0.4722 0.2872 0.2349]\n",
            " [0.4722 0.3394 0.2819 0.2872]\n",
            " [0.2872 0.2819 0.3394 0.4722]\n",
            " [0.2349 0.2872 0.4722 1.    ]]\n",
            "\n",
            "\n",
            "Iteration: 200, grid:\n",
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exercise assumes that this episodic task doesn't have a discount factor and that's why the robot doesn't \"improve\", i.e. find an optimal path to the exit. We can see this in the case where $\\gamma = 1$ - there is no discount and all of the states yield the same reward which is $1$. We can see this by writing out the return at time $t$:\n",
        "\n",
        "$$G_t = R_{t+1} + R_{t+2} +...+R_{T} = R_{T} = 1$$\n",
        "\n",
        "So this means that at each time step, given we use the greedy policy, we would pick a random state to go to (because each state would be the maximum). This makes the agent not have a designated path towards the exit and it will just go in circles, choosing random actions, until it randomly takes an exit (goes into the terminal state).\n",
        "\n",
        "On the other hand when $\\gamma \\in (0,1)$ the return at time step $t$ will be \n",
        "\n",
        "$$G_t = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-t-1} R_T = R_T\\gamma^{T-t-1} = (1)\\gamma^{T-t-1}=\\gamma^{T-t-1}$$ \n",
        "\n",
        "We can see that if $t$ is close to $T$, then the reward is bigger. E.g. let $\\gamma = 0.9$, $T=5$ and $t_1= 2$ and $t_2=4$ ($t_1$ and $t_2$ are time steps in different episodes), then\n",
        "\n",
        " $$G_{t_1} = G_2 = \\gamma^{5-2-1}= \\gamma^{2} = 0.9^2 = 0.81 < G_{t_2} = G_4 = \\gamma^{5-4-1} = \\gamma^{0} = 1$$\n",
        "\n",
        "This means that the states which are closer to the terminal state clearly get to the terminal state faster, hence their value goes up. \n",
        "\n",
        "Another way of solving this problem is to change the rewards themselves, so that at each time step, the reward is $-1$ and at the terminal state it's $0$. So this will \"punish\" the agent for staying long in the maze, just like with the case where we have discounts. In the following code you will see that this is indeed the case."
      ],
      "metadata": {
        "id": "si8UMO86sRud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maze(s):\n",
        "  maze = -1*np.ones((s, s))\n",
        "  maze[0,0], maze[-1,-1] = 0, 0\n",
        "  return maze\n",
        "print(maze(4))\n",
        "\n",
        "def robot_experiment(iterations, discount, maze_size):\n",
        "  ITERATIONS = iterations\n",
        "  grid = maze(maze_size)\n",
        "  gridCollection = [maze(4)]\n",
        "  probs = np.array([0.25, 0.25, 0.25, 0.25])\n",
        "  for iter in range(ITERATIONS):\n",
        "      for row in range(grid.shape[0]):\n",
        "        for col in range(grid.shape[1]):\n",
        "          index = [row, col]\n",
        "          rowID, colID = findCells(index, grid)\n",
        "          if rowID == None or colID == None:\n",
        "            grid[row, col] = grid[row, col]\n",
        "          else:\n",
        "            last_grid = gridCollection[iter]\n",
        "            grid[row, col] = discount*last_grid[rowID, colID] @ probs\n",
        "      new_grid = grid.copy()\n",
        "      gridCollection.append(new_grid)\n",
        "      if (iter+1)% iterations==0:\n",
        "        print(f\"Iteration: {iter+1}, grid:\")\n",
        "        print(grid)\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opIPApMvvEut",
        "outputId": "39c6cf61-7f94-4e38-ae72-0394e8924b31"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "robot_experiment(200, 0.1, 4)\n",
        "robot_experiment(200, 0.9, 4)\n",
        "robot_experiment(200, 1, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAojaSvfvQmn",
        "outputId": "8f90e3a0-fbbe-4e99-ca06-6369c03669e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 200, grid:\n",
            "[[ 0.00000000e+000 -1.29049831e-205 -1.91228610e-205 -2.13996124e-205]\n",
            " [-1.29049831e-205 -1.68461097e-205 -1.89949420e-205 -1.91228610e-205]\n",
            " [-1.91228610e-205 -1.89949420e-205 -1.68461097e-205 -1.29049831e-205]\n",
            " [-2.13996124e-205 -1.91228610e-205 -1.29049831e-205  0.00000000e+000]]\n",
            "\n",
            "\n",
            "Iteration: 200, grid:\n",
            "[[ 0.00000000e+00 -9.10456764e-15 -1.34913297e-14 -1.50975958e-14]\n",
            " [-9.10456764e-15 -1.18850636e-14 -1.34010818e-14 -1.34913297e-14]\n",
            " [-1.34913297e-14 -1.34010818e-14 -1.18850636e-14 -9.10456764e-15]\n",
            " [-1.50975958e-14 -1.34913297e-14 -9.10456764e-15  0.00000000e+00]]\n",
            "\n",
            "\n",
            "Iteration: 200, grid:\n",
            "[[ 0.00000000e+00 -1.29049831e-05 -1.91228610e-05 -2.13996124e-05]\n",
            " [-1.29049831e-05 -1.68461097e-05 -1.89949420e-05 -1.91228610e-05]\n",
            " [-1.91228610e-05 -1.89949420e-05 -1.68461097e-05 -1.29049831e-05]\n",
            " [-2.13996124e-05 -1.91228610e-05 -1.29049831e-05  0.00000000e+00]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}