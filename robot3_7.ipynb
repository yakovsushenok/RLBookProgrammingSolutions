{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "robot3_7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I decided to write this code to check how exactly an agent behaves when the rewards for transitions in a maze are equal to $0$ and the reward for escaping the maze is $+1$.\n",
        "\n",
        "This is to answer the following question from Sutton and Barto's book:\n",
        "\n",
        "Exercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a\n",
        "reward of +1 for escaping from the maze and a reward of zero at all other times. The task\n",
        "seems to break down naturally into episodes—the successive runs through the maze—so\n",
        "you decide to treat it as an episodic task, where the goal is to maximize expected total\n",
        "reward (3.7). After running the learning agent for a while, you find that it is showing\n",
        "no improvement in escaping from the maze. What is going wrong? Have you e↵ectively\n",
        "communicated to the agent what you want it to achieve?"
      ],
      "metadata": {
        "id": "1U5EhrZVR-Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's create a matrix which will represent our maze. The 2 corners at the top left and bottom right are the exits of the maze. If the agent steps into the exit, then the episode terminates."
      ],
      "metadata": {
        "id": "J2F9NHucTggJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def maze(s):\n",
        "  maze = np.zeros((s, s))\n",
        "  maze[0,0], maze[-1,-1] = 1, 1\n",
        "  return maze"
      ],
      "metadata": {
        "id": "hfGB73NzSJTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how a $4 \\times 4$ maze looks like:"
      ],
      "metadata": {
        "id": "_urS1xS3SNNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(maze(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQUnSO8xSP72",
        "outputId": "8634a4b2-cdb8-4af4-f9ff-33c0f5572ce1"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's calculate the state values for the uniform policy. Note that if you're on the edge and you turn into the wall then you circle back to the same state. \n",
        "\n",
        "Under the uniform policy each action has a probability of $\\frac{1}{4}$ to be taken. We calculate the state values under the uniform policy because it's interesting to see what will the estimates be with such rewards. The algorithm for evaluating the state values is:\n",
        "\n",
        "First, initialise $v_0=0$, then iterate $$\\forall s: v_{k+1}(s)\\leftarrow E[R_{t+1}+\\gamma v_k (S_{t+1})|s,\\pi ]$$\n",
        "\n"
      ],
      "metadata": {
        "id": "5IjK-UCnUNGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findCells(index, grid):\n",
        "  \"\"\"\n",
        "  A function which will get the cells values neighboring a specific cell. \n",
        "  input:\n",
        "      index: array of 2 entries i and j which indicate the cell's position. i is the row and j is the column\n",
        "  ouput: \n",
        "      cells: 2 arrays. The first array has the row values of the neighbouring cells and the second array the column values. \n",
        "  Example:\n",
        "   input: [1,1]\n",
        "   output: [1, 0, 1, 2], [0, 1, 2, 1]\n",
        "  \"\"\"\n",
        "  i, j = index[0], index[1]\n",
        "  # Terminal state\n",
        "  if (np.all(grid[i, :] == grid[0, :]) and np.all(grid[:, j] == grid[:, 0])) or (np.all(grid[i, :] == grid[-1, :]) and np.all(grid[:, j] == grid[:, -1])):\n",
        "    return None, None\n",
        "  # Top wall, not corner\n",
        "  elif np.all(grid[i, :] == grid[0, :]) and np.all(grid[:, j] == grid[:,-1]) == False:\n",
        "    return [i, i, i, i+1], [j-1, j, j+1, j]\n",
        "  # Top wall, corner\n",
        "  elif np.all(grid[i, :] == grid[0, :]) and np.all(grid[:, j] == grid[:,-1]):\n",
        "    return [i, i, i, i+1], [j-1, j, j, j]\n",
        "  # Bottom wall, not corner\n",
        "  elif np.all(grid[i, :] == grid[-1, :]) and np.all(grid[:, j] == grid[:, 0])==False:\n",
        "    return [i, i-1, i, i], [j-1, j, j+1, j]\n",
        "  # Bottom wall, corner\n",
        "  elif np.all(grid[i, :] == grid[-1, :]) and np.all(grid[:, j] == grid[:, 0]):\n",
        "    return [i, i-1, i, i], [j, j, j+1, j]\n",
        "  # Left wall, not corner\n",
        "  elif np.all(grid[:, j] == grid[:, 0]) and np.all(grid[i,:]== grid[-1,:])==False:\n",
        "    return [i, i-1, i, i+1], [j, j, j+1, j]\n",
        "  # Right wall, not corner\n",
        "  elif np.all(grid[:, j] == grid[:, -1]) and np.all(grid[i, :] == grid[-1,:])==False:\n",
        "    return [i, i-1, i, i+1], [j-1, j, j, j]\n",
        "  # No wall\n",
        "  else:\n",
        "    return [i, i-1, i, i+1], [j-1, j, j+1, j]\n",
        "  \n",
        "def robot_experiment(iterations, discount, maze_size):\n",
        "  ITERATIONS = iterations\n",
        "  grid = maze(maze_size)\n",
        "  gridCollection = [maze(4)]\n",
        "  probs = np.array([0.25, 0.25, 0.25, 0.25])\n",
        "  for iter in range(ITERATIONS):\n",
        "      for row in range(grid.shape[0]):\n",
        "        for col in range(grid.shape[1]):\n",
        "          index = [row, col]\n",
        "          rowID, colID = findCells(index, grid)\n",
        "          if rowID == None or colID == None:\n",
        "            grid[row, col] = grid[row, col]\n",
        "          else:\n",
        "            last_grid = gridCollection[iter]\n",
        "            grid[row, col] = discount*last_grid[rowID, colID] @ probs\n",
        "      new_grid = grid.copy()\n",
        "      gridCollection.append(new_grid)\n",
        "      if (iter+1)% iterations==0:\n",
        "        print(f\"Iteration: {iter+1}, grid:\")\n",
        "        print(np.round(grid, 4))\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "LBTBdtOpTKnO"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am now going to see what the state values converge to. I will try values with a discount $\\gamma \\in \\{0.1,0.9,1\\}$. I will do 200 iterations as it's enough for convergence."
      ],
      "metadata": {
        "id": "FSkw0YMxPrp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "robot_experiment(200, 0.1, 4)\n",
        "robot_experiment(200, 0.9, 4)\n",
        "robot_experiment(200, 1, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnwlltGOP4Sy",
        "outputId": "054014c1-3eae-424a-87b2-9972cf07f137"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 200, grid:\n",
            "[[1.00e+00 2.57e-02 7.00e-04 0.00e+00]\n",
            " [2.57e-02 1.30e-03 1.00e-04 7.00e-04]\n",
            " [7.00e-04 1.00e-04 1.30e-03 2.57e-02]\n",
            " [0.00e+00 7.00e-04 2.57e-02 1.00e+00]]\n",
            "\n",
            "\n",
            "Iteration: 200, grid:\n",
            "[[1.     0.4722 0.2872 0.2349]\n",
            " [0.4722 0.3394 0.2819 0.2872]\n",
            " [0.2872 0.2819 0.3394 0.4722]\n",
            " [0.2349 0.2872 0.4722 1.    ]]\n",
            "\n",
            "\n",
            "Iteration: 200, grid:\n",
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the policies with discounts give states closer to the exit higher values as opposed to the values in the scenario where there is no discount. This is due to the fact that the closer the cell is to the exit, the sooner it can get a reward of $+1$ without it being discounted too much as opposed to the cells that are further away. We can see that the scenario with no discount has all of its state values converged to $1$. This should make sense since when the robot starts at some cell (state), it will eventually get to the exit and get a reward of $+1$, whever it is after $2$ steps of after $1000$ steps. \n",
        "\n",
        "I think that a useful insight I gained from this experiment is that with the discount factor, the state values converged to values which made it clear which path to take e.g if you were to employ a greedy policy, it would know momentarily which actions to take. With the scenario where we don't have a discount, if we would employ a greedy policy we would have to optimize it. Hence, the discount served as a \"punishment\" for not escaping the exit quickly. \n",
        "\n",
        "An alternative way to encourage the agent to escape the maze as quickly as possible is to put a reward of $-1$ for a transition. This way the robot will maximize its return by escaping the maze as quickly as possible. FUTURE WORK: WRITE AN ALGORITHM WHICH CALCULATE $V^*$ and $\\pi^*$"
      ],
      "metadata": {
        "id": "BIjHjEWmSE_v"
      }
    }
  ]
}
